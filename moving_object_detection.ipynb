{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libarries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from segment_anything import sam_model_registry, SamPredictor, SamAutomaticMaskGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## path and model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path  = os.path.join(os.getcwd(), 'src','classroom.mp4')\n",
    "\n",
    "## checkpoints for sam\n",
    "sam_checkpoints = \"checkpoints\"\n",
    "vit_h = \"sam_vit_h_4b8939.pth\"\n",
    "vit_b = \"sam_vit_b_01ec64.pth\"\n",
    "vit_l = \"sam_vit_l_0b3195.pth\"\n",
    "\n",
    "## check for device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## yolo model\n",
    "yolo_model = YOLO('yolov8x.pt').to(device)\n",
    "\n",
    "## sam model\n",
    "model_type = \"vit_l\"\n",
    "sam = sam_model_registry[model_type](checkpoint=os.path.join(sam_checkpoints, vit_l))\n",
    "sam = sam.to(device)\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(img):\n",
    "    augmented_images = [\n",
    "        img,\n",
    "        ## apply gaussian blur\n",
    "        transforms.GaussianBlur(kernel_size=3)(img),\n",
    "        ## change the hue of the image\n",
    "        transforms.ColorJitter(hue=0.5)(img),\n",
    "        ## change the saturation of the image\n",
    "        transforms.ColorJitter(saturation=0.5)(img),\n",
    "        ## change, make the image blur, then apply gaussian blur\n",
    "        transforms.GaussianBlur(kernel_size=3)(transforms.ColorJitter(brightness=0.5)(img)),\n",
    "    ]\n",
    "    \n",
    "    #img_batch = torch.stack(augmented_images)\n",
    "    \n",
    "    augmented_images = [np.array(img.permute(2,1,0)) for img in augmented_images]\n",
    "    return augmented_images\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(os.getcwd(), 'src','test2.jpg')\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "\n",
    "## resize image to 416x416\n",
    "#img = cv2.resize(img, (1024, 2336), interpolation=cv2.INTER_LINEAR)\n",
    "## convert image to tensor\n",
    "img = torch.from_numpy(img).permute(2, 1, 0).float()\n",
    "\n",
    "image_list = augment_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x448 2 persons, 1: 640x448 3 persons, 2: 640x448 2 persons, 3: 640x448 (no detections), 4: 640x448 (no detections), 166.0ms\n",
      "Speed: 265.0ms preprocess, 33.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n"
     ]
    }
   ],
   "source": [
    "resutls = yolo_model(image_list, conf=0.10, classes=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame):\n",
    "    \n",
    "    results = yolo_model(frame, conf=0.25, classes=[0])\n",
    "    \n",
    "    ## Process results\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        \n",
    "    bbox = boxes.xyxy\n",
    "    print('bbox shape: ', bbox.shape)\n",
    "    #confidences = boxes.conf\n",
    "    #classes = boxes.cls \n",
    "    #predictor = SamPredictor(sam)\n",
    "    predictor.set_image(frame)\n",
    "    \n",
    "    input_boxes = bbox.to(predictor.device)\n",
    "    transformed_boxes = predictor.transform.apply_boxes_torch(input_boxes, frame.shape[:2])\n",
    "    \n",
    "    masks, _, _ = predictor.predict_torch(\n",
    "    point_coords=None,\n",
    "    point_labels=None,\n",
    "    boxes=transformed_boxes,\n",
    "    multimask_output=False,\n",
    "    )\n",
    "    \n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_mask2img(mask):\n",
    "    palette = {\n",
    "        0: (0, 0, 0),\n",
    "        1: (255, 0, 0),\n",
    "        2: (0, 255, 0),\n",
    "        3: (0, 0, 255),\n",
    "        4: (0, 255, 255),\n",
    "    }\n",
    "    items = mask.shape[0]\n",
    "    rows = mask.shape[1]\n",
    "    cols = mask.shape[2]\n",
    "    image = np.zeros((items, rows, cols, 3), dtype=np.uint8)\n",
    "    image[:, :, :, 0] = mask * palette[1][0]\n",
    "    image[:, :, :, 1] = mask * palette[1][1]\n",
    "    image[:, :, :, 2] = mask * palette[1][2]\n",
    "    return image\n",
    "\n",
    "def optimized_show_mask(masks):\n",
    "    masks = np.squeeze(masks, axis = 1)\n",
    "    separate_rgb_masks = optimized_mask2img(masks)\n",
    "    combined_mask = np.sum(separate_rgb_masks, axis = 0)\n",
    "    return combined_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 8 persons, 69.0ms\n",
      "Speed: 7.0ms preprocess, 69.0ms inference, 12.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbox shape:  torch.Size([8, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks shape:  (8, 480, 852) 8 [0 1]\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "bbox shape:  torch.Size([7, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks shape:  (7, 480, 852) 7 [0 1]\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "bbox shape:  torch.Size([7, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 8 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks shape:  (7, 480, 852) 7 [0 1]\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "bbox shape:  torch.Size([8, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks shape:  (8, 480, 852) 8 [0 1]\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "bbox shape:  torch.Size([7, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks shape:  (7, 480, 852) 7 [0 1]\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "bbox shape:  torch.Size([7, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks shape:  (7, 480, 852) 7 [0 1]\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "bbox shape:  torch.Size([7, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks shape:  (7, 480, 852) 7 [0 1]\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "bbox shape:  torch.Size([7, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks shape:  (7, 480, 852) 7 [0 1]\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "bbox shape:  torch.Size([7, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 8 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks shape:  (7, 480, 852) 7 [0 1]\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "bbox shape:  torch.Size([8, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 8 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks shape:  (8, 480, 852) 8 [0 1]\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "bbox shape:  torch.Size([8, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks shape:  (8, 480, 852) 8 [0 1]\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "bbox shape:  torch.Size([7, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 8 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks shape:  (7, 480, 852) 7 [0 1]\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "bbox shape:  torch.Size([8, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 8 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masks shape:  (8, 480, 852) 8 [0 1]\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "bbox shape:  torch.Size([8, 4])\n",
      "masks shape:  (8, 480, 852) 8 [0 1]\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n",
      "in shape:  (480, 852)\n"
     ]
    }
   ],
   "source": [
    "## Load YOLO\n",
    "## yolo model\n",
    "yolo_model = YOLO('yolov8x.pt').to(device)\n",
    "\n",
    "## sam model\n",
    "model_type = \"vit_h\"\n",
    "sam = sam_model_registry[model_type](checkpoint=os.path.join(sam_checkpoints, vit_h))\n",
    "sam = sam.to(device)\n",
    "predictor = SamPredictor(sam)\n",
    "    \n",
    "    \n",
    "    \n",
    "## Load video\n",
    "video_path  = os.path.join(os.getcwd(), 'src','classroom.mp4')\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "#cap = cv2.VideoCapture(0)\n",
    "    \n",
    "if cap.isOpened() == False:\n",
    "    print(\"Error in loading the video\")\n",
    "    \n",
    "i = 0\n",
    "    \n",
    "# # Get the video properties\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "fps = int(cap.get(5))\n",
    "\n",
    "# # Define the output video path\n",
    "output_path_contours = os.path.join(os.getcwd(), 'output', 'classroom_c.mp4')\n",
    "output_path_segmentation = os.path.join(os.getcwd(), 'output', 'classroom_s.mp4')\n",
    "\n",
    "# # Create a VideoWriter object to save the processed frames\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out_c = cv2.VideoWriter(output_path_contours, fourcc, fps, (frame_width, frame_height))\n",
    "out_s = cv2.VideoWriter(output_path_segmentation, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    try:\n",
    "        # frame2 = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        #frame2 = cv2.cvtColor((cv2.GaussianBlur(frame2, (3, 3), 0)), cv2.COLOR_GRAY2RGB)\n",
    "        masks = process_frame(frame, yolo_model, predictor)\n",
    "            #dispaly frame and colour mask in same window\n",
    "            \n",
    "         \n",
    "        if masks is not None:        \n",
    "            frame = ((frame/np.max(frame))*255).astype(np.uint8)\n",
    "            colour_mask = optimized_show_mask(masks.detach().cpu().numpy())\n",
    "       \n",
    "            colour_mask = cv2.addWeighted(colour_mask.astype(np.uint8), 0.3, frame, 0.7, 0, dtype=cv2.CV_8U)#colour_mask.astype(np.uint8))\n",
    "                \n",
    "            #-----------for contours -------\n",
    "            masks = np.squeeze(masks.detach().cpu().numpy(), axis = 1).astype(np.uint8)\n",
    "            #print('masks shape: ', masks.shape, masks.shape[0], np.unique(masks))\n",
    "            for dim in range(masks.shape[0]):\n",
    "                #print('in shape: ', masks[dim, :, :].shape)\n",
    "                contours, hierarchy = cv2.findContours(image = masks[dim, :, :], mode = cv2.RETR_TREE, method = cv2.CHAIN_APPROX_NONE)\n",
    "                cv2.drawContours(image = frame, contours=contours, contourIdx=-1, color=(0, 255, 0), thickness=1, lineType=cv2.LINE_AA)\n",
    "        \n",
    "            #cv2.imshow('frame', frame)\n",
    "            #cv2.imshow('frame', colour_mask)\n",
    "        \n",
    "            # Write the combined frame to the output video\n",
    "            out_c.write(frame)\n",
    "            out_s.write(colour_mask)\n",
    "        else:\n",
    "            out_c.write(frame)\n",
    "            out_s.write(frame)\n",
    "    \n",
    "        # if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                #break\n",
    "    \n",
    "        i = i + 1\n",
    "    ## save frame and make video\n",
    "    except:\n",
    "        i = i + 1\n",
    "        out_c.release()\n",
    "        out_s.release()\n",
    "        break\n",
    "       \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
